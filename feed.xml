<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://joyoshish.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joyoshish.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-12T12:49:34+00:00</updated><id>https://joyoshish.github.io/feed.xml</id><title type="html">blank</title><subtitle>Joyoshish&apos;s Digital Space </subtitle><entry><title type="html">k-means++ অ্যালগরিদম</title><link href="https://joyoshish.github.io/blog/2024/kmeansplusplus_beng/" rel="alternate" type="text/html" title="k-means++ অ্যালগরিদম"/><published>2024-05-05T10:00:00+00:00</published><updated>2024-05-05T10:00:00+00:00</updated><id>https://joyoshish.github.io/blog/2024/kmeansplusplus_beng</id><content type="html" xml:base="https://joyoshish.github.io/blog/2024/kmeansplusplus_beng/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp-480.webp 480w,/assets/img/kmpp-800.webp 800w,/assets/img/kmpp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Unlucky choices! (Source: http://shabal.in/visuals.html) </div> <p>k-means++ clustering ব্যাবহার করে আমরা বেশিরভাগ unsupervised learning এর ক্ষেত্রে k-means clustering এর তুলনায় বেশি দ্রুত এবং অধিকতর নিকট সমাধান পেতে পারি। যেহেতু unsupervised learning এর ক্ষেত্রে কোনও সুনির্দিষ্ট সমাধান থাকে না, তাই যেকোনো unsupervised learning এই আমরা চেষ্টা করি cost কমানোর যেটি k-means++ খুবই দক্ষতার সাথে করে।</p> <p>David Arthur এবং Sergei Vassilvitskii ২০০৭ সালে k-means++ অ্যালগরিদমটির প্রস্তাব দেন standard k-means থেকে clustering এর NP-hard সমাধান হিসাবে।</p> <h3 id="standard-k-means-algorithm-এর-অপূর্ণতা">Standard k-means algorithm এর অপূর্ণতা</h3> <p>k-means++ শেখার আগে আমাদের জেনে নেওয়া ভাল standard k-means algorithm এর ত্রুটি এবং এর অপূর্ণতাগুলি।</p> <p>১/ কিছু ক্ষেত্রে k-means বহুবার iterated হয় যা algorithm টাকে ধীর করে দেয়।</p> <p>২/ k-means কেমন কাজ করবে তা বেশিরভাগটাই নির্ভর করে আমরা প্রাথমিক ভাবে যে cluster center গুলো এলোমেলোভাবে নির্বাচন করেছি তার উপর। এটির কারণেই কখনো কখনো k-means একই cluster এ দুটি cluster center নির্বাচন করে ফেলে। এটি উপযুক্ত উদাহরণ দিয়ে বোঝা যাক।</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp1.webp-480.webp 480w,/assets/img/kmpp1.webp-800.webp 800w,/assets/img/kmpp1.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>এক্ষেত্রে ‘a’ cluster এ বাস্তবে 2 টি cluster থাকলেও k-means তা নির্ধারণ করতে অক্ষম। একইসাথে এটি ‘c’ ও ‘d’ cluster কে আলাদা বললেও তা আসলে একটিই cluster। এভাবেই k-means কখনো কখনো local optimum এই সীমাবদ্ধ থেকে যায় এবং তা bad cluster সৃষ্টি করে।</p> <h3 id="k-means-algorithm">k-means++ algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp2.webp-480.webp 480w,/assets/img/kmpp2.webp-800.webp 800w,/assets/img/kmpp2.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>এবার আসা যাক k-means++ algorithm এ এবং এটি কেমন কার্যকরী ভাবে clustering করে।</p> <p>কোনো data set থেকে আমরা নিম্নোক্ত উপায়ে clustering করতে পারি।</p> <p>১ক/ প্রথমেই আমরা randomly একটি cluster center নির্ধারণ করি data set থেকে। (uniform and random choice)</p> <p>১খ/ এখন data set এর থেকে পরবর্তী center টি নির্ধারণ করতে আমরা furthest point strategy ব্যাবহার করি।</p> <p>এই পদ্ধতিতে আমরা কোনো point কে তখনি একটি cluster center নির্বাচন করি যখন সেটি পূর্ব নির্ধারিত cluster center থেকে সবথেকে বেশি দুরে অবস্থান করে।</p> <p>এটি গাণিতিকভাবে নির্ণয় করতে আমরা একটি নতুন চল ধরব D(x) যা হল কোনো data point থেকে ইতিমধ্যে নির্বাচিত cluster center গুলোর দূরত্ব গুলোর মধ্যে নিম্নতমটি (min c∈C abs(x − c))। এখন পরবর্তী cluster center নির্ধারণ করার সময় আমরা data point গুলোর মধ্যে সেটিকে নির্বাচন করব যার ক্ষেত্রে D(x)²/ΣD(x)² এই সম্ভাবনা টা বেশি। (furthest point strategy)</p> <p>নীচে একটি উদাহারণ দিলে বিষয়টি আরও পরিষ্কার হয়ে যাবে। (ক্রমশ)</p> <p>২/ আমরা এই ভাবে cluster center নির্বাচন করব যতক্ষণ আমাদের কাছে k টি cluster center আসে।</p> <p>৩/ এভাবে cluster center initialisation এর পর আমরা প্রত্যেক data point কে তার নিকটতম cluster center এর সংশ্লিষ্ট cluster এ অন্তর্ভুক্ত করবো, যেমন ভাবে আমরা k-means algorithm এ করে থাকি।</p> <p>আমরা এই data points allocation ততক্ষণ করবো যতক্ষণ না cluster center গুলো অপরিবর্তিত থাকছে।</p> <h3 id="উদাহরণ">উদাহরণ</h3> <p>এবার একটি 1 dimensional data points নিয়ে k-means++ algorithm টি ঝালিয়ে নেওয়া যাক।</p> <p>ধরা যাক আমাদের কাছে data points আছে {0, 1, 2, 5, 6} এবং আমরা ২ টি cluster এই data points থেকে পেতে চাই। (k = 2)</p> <p>প্রথম cluster center C1 হল 0। C1 = 0 (প্রথম cluster center টি আমরা অনির্দিষ্ট ভাবে প্রথম data point টাকে নেব)</p> <p>এখন দ্বিতীয় cluster center নির্বাচনের ক্ষেত্রে আমরা D(x)²/ΣD(x)² এই সম্ভাবনা টা প্রতিটি data point এর জন্য নির্ণয় করবো।</p> <p>P(C2=1) = ((1–0)²)f = 1f নিকটতম cluster center, C1=0</p> <p>P(C2=2) = ((2–0)²)f = 4f নিকটতম cluster center, C1=0</p> <p>P(C2=5) = ((5–0)²)f = 25f নিকটতম cluster center, C1=0</p> <p>P(C2=6) = ((6–0)²)f = 36f নিকটতম cluster center, C1=0</p> <p>এখানে f = (1+4+25+36)</p> <p>দেখা যাচ্ছে P(C2=6) সবথেকে বেশি, তাই k-means++ 6 কে C2 হিসাবে নির্বাচন করবে। C2 = 6</p> <p>এরপর আমরা data point গুলোকে সবথেকে কাছের সংশ্লিষ্ট cluster center এর সাথে যুক্ত করলেই দুটি cluster পেয়ে যাব {(0, 1, 2)(5, 6)} ।</p> <p>অন্য একটি উদাহরণ নিয়ে দেখা যাক।</p> <p>ধরা যাক আমাদের কাছে data points আছে</p> <p>{0, 1, 2, 5, 6, 9, 10} এবং আমরা ৩ টি cluster এই data points থেকে পেতে চাই। (k = 3)</p> <p>প্রথম cluster center C1 হল 0। C1 = 0</p> <p>দ্বিতীয় cluster center নির্বাচনের ক্ষেত্রে প্রথম উদাহরণের মতই আমরা furthest point 10 কে C2 হিসাবে নির্বাচন করবো। C2 = 10</p> <p>এখন তৃতীয় cluster center এর জন্য আমাদেরকে সংশ্লিষ্ট সম্ভাবনা গুলো নির্ণয় করতে হবে প্রতিটি data point এর জন্য।</p> <p>P(C3=1) = ((1–0)²)f = 1f নিকটতম cluster center, C1=0</p> <p>P(C3=2) = ((2–0)²)f = 4f নিকটতম cluster center, C1=0</p> <p>P(C3=5) = ((5–0)²)f = 25f নিকটতম cluster center, C1=0</p> <p>P(C3=6) = ((6–10)²)f = 16f নিকটতম cluster center, C2=10</p> <p>P(C3=9) = ((9–10)²)f = 1f নিকটতম cluster center, C2=10</p> <p>এখানে f = (1+4+25+16+1)</p> <p>দেখা যাচ্ছে P(C3=5) সবথেকে বেশি, তাই k-means++ 5 কে C3 হিসাবে নির্বাচন করবে। C3 = 5</p> <p>এরপর আমরা data point গুলোকে সবথেকে কাছের সংশ্লিষ্ট cluster center এর সাথে সংযুক্ত করলেই তিনটি cluster পেয়ে যাব। {(0, 1, 2)(5, 6)(9, 10)}</p> <p>এভাবেই 2 dimensional বা উচ্চতর dimension এ আমরা k-means++ ব্যাবহার করে clustering করতে পারি। তথ্যগতভাবে বলা যেতে পারে k-means এবং k-means++ এর মধ্যে cluster center initialisation এরই শুধু পার্থক্য রয়েছে, বাকি প্রক্রিয়া দুক্ষেত্রেই এক।</p> <h3 id="k-means-algorithm-এর-কার্যকারিতা">k-means++ algorithm এর কার্যকারিতা</h3> <p>এখন k-means algorithm এর থেকে k-means++ কতটা কার্যকর তা দেখা যাক।</p> <p>এখানে একটি dataset এর scatter plot দেওয়া হল</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp3.webp-480.webp 480w,/assets/img/kmpp3.webp-800.webp 800w,/assets/img/kmpp3.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp3.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>এবার k-means এবং k-means++ এর জন্য cluster center initialisation এর 2 dimensional histogram দেখা যাক</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp4.webp-480.webp 480w,/assets/img/kmpp4.webp-800.webp 800w,/assets/img/kmpp4.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp4.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>এর থেকে পরিষ্কার ভাবে বোঝা যায় k-means++ cluster গুলোর মধ্যভাগে বেশি initialise করেছে যা বেশিরভাগ dataset এর জন্য দ্রুত হবে এবং অধিকতর সঠিক cluster বানাতে সক্ষম হবে।</p> <h3 id="k-means-এর-অপূর্ণতা">k-means++ এর অপূর্ণতা</h3> <p>K-means++ সম্পর্কে আলোচনা শেষ করার পূর্বে এটির কিছু ত্রুটি দেখে নেওয়া দরকার যা বাস্তব data set গুলোতে ব্যাবহারের ক্ষেত্রে দেখা দিতে পারে।</p> <p>১/ k-means++ বিচ্ছিন্ন data point এর ক্ষেত্রে খুবই সংবেদনশীল (sensitive to outliers)। কারণ k-means++ সবসময় furthest point strategy ব্যাবহার করে cluster center গুলো নির্বাচন করে, যা করতে গিয়ে কিছুক্ষেত্রে তা বিচ্ছিন্ন একটি point কে cluster center নির্বাচন করে ফেলে যা কোন cluster সৃষ্টি করতে তেমন কোন গুরুত্ব রাখে না।</p> <p>২/ ধরা যাক কোন Game Developer Company তাদের game খেলে, এমন যত gamer আছে, তাদের পছন্দের game genre(List of video game genres — Wikipedia) গুলো দিয়ে clustering করতে আগ্রহী। এখন game genre এর সংখ্যা অনেক বেশি হওয়ায় k-means++ cluster center initialisation এর জন্য ঠিক ততবারই data point গুলোর মধ্যে algorithm টিকে pass করাবে। সেক্ষেত্রে প্রক্রিয়াটি মন্থর হয়ে যাবে। k-means++ k সংখ্যক বার data point pass করার জন্য যেখানে k একটি অনেক বড় সংখ্যা তখন দ্রুত কাজ করে না। এটি অতিক্রম করতে আমরা oversampling এর সাহায্য নিই এবং এর থেকে k-means এর অন্য একটি ভিন্নতা — parallel k-means এর উৎপত্তি (<a href="http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf">http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf</a>)।</p> <p>এসব সত্ত্বেও বেশিরভাগ data set এর জন্য k-means++ তথাকথিত k-means algorithm এর থেকে বেশি সফল ভাবে clustering করতে সক্ষম।</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="ml"/><category term="ai"/><summary type="html"><![CDATA[k-means++ অ্যালগরিদম]]></summary></entry><entry><title type="html">k-means++ অ্যালগরিদম</title><link href="https://joyoshish.github.io/blog/2020/k-means/" rel="alternate" type="text/html" title="k-means++ অ্যালগরিদম"/><published>2020-08-07T15:27:12+00:00</published><updated>2020-08-07T15:27:12+00:00</updated><id>https://joyoshish.github.io/blog/2020/k-means-</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/k-means/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">মেশিন লার্নিং-এর কলকব্জা ৩ : Linear Regression ও Gradient Descent</title><link href="https://joyoshish.github.io/blog/2020/linear-regression-gradient-descent/" rel="alternate" type="text/html" title="মেশিন লার্নিং-এর কলকব্জা ৩ : Linear Regression ও Gradient Descent"/><published>2020-08-04T09:19:46+00:00</published><updated>2020-08-04T09:19:46+00:00</updated><id>https://joyoshish.github.io/blog/2020/-----linear-regression--gradient-descent</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/linear-regression-gradient-descent/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">মেশিন লার্নিং-এর কলকব্জা ২ : ক্ষয়ক্ষতির হিসেব</title><link href="https://joyoshish.github.io/blog/2020/" rel="alternate" type="text/html" title="মেশিন লার্নিং-এর কলকব্জা ২ : ক্ষয়ক্ষতির হিসেব"/><published>2020-08-03T03:17:30+00:00</published><updated>2020-08-03T03:17:30+00:00</updated><id>https://joyoshish.github.io/blog/------</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">মেশিন লার্নিং-এর কলকব্জা ১ : যন্ত্র কতভাবে শেখে</title><link href="https://joyoshish.github.io/blog/2020/" rel="alternate" type="text/html" title="মেশিন লার্নিং-এর কলকব্জা ১ : যন্ত্র কতভাবে শেখে"/><published>2020-08-03T03:15:42+00:00</published><updated>2020-08-03T03:15:42+00:00</updated><id>https://joyoshish.github.io/blog/-------</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">চলো যন্ত্রকে ফুল চেনাই</title><link href="https://joyoshish.github.io/blog/2020/" rel="alternate" type="text/html" title="চলো যন্ত্রকে ফুল চেনাই"/><published>2020-08-03T03:14:01+00:00</published><updated>2020-08-03T03:14:01+00:00</updated><id>https://joyoshish.github.io/blog/---</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">মেশিন লার্নিং : উপক্রমণিকা</title><link href="https://joyoshish.github.io/blog/2020/" rel="alternate" type="text/html" title="মেশিন লার্নিং : উপক্রমণিকা"/><published>2020-08-02T04:52:54+00:00</published><updated>2020-08-02T04:52:54+00:00</updated><id>https://joyoshish.github.io/blog/--</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/"><![CDATA[]]></content><author><name></name></author></entry></feed>